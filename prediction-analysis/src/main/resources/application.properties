# Prediction Analysis Module Configuration

# Server configuration
quarkus.http.port=8083

# Redis configuration (shared with other modules)
# Dev services will handle Redis in dev mode, only set for production
%dev.quarkus.redis.devservices.enabled=true
%dev.quarkus.hibernate-orm.database.generation=update
%prod.quarkus.redis.hosts=${REDIS_URL:redis://localhost:6379}

# Prediction Extractor Configuration
prediction.extractor.type=gemini-native
prediction.extractor.auto-fallback=true
prediction.extractor.prefer-llm=true

# Mock Prediction Extractor Configuration
prediction.mock.enabled=true
prediction.mock.processing-delay-ms=500
prediction.mock.min-predictions=1
prediction.mock.max-predictions=5
prediction.mock.min-confidence=0.3
prediction.mock.max-confidence=0.95

# LLM Configuration - Gemini as default
prediction.llm.enabled=true
prediction.llm.provider=gemini-native
prediction.llm.model=gemini-2.5-flash-lite
prediction.llm.api-key=${GEMINI_API_KEY:}

# LLM Request Configuration
prediction.llm.timeout-seconds=120
prediction.llm.max-tokens=4000
prediction.llm.temperature=0.3
prediction.llm.top-p=1.0
prediction.llm.frequency-penalty=0.0
prediction.llm.presence-penalty=0.0

# LLM Retry and Rate Limiting
prediction.llm.retry-attempts=3
prediction.llm.retry-delay-seconds=1
prediction.llm.rate-limit-per-minute=10
prediction.llm.rate-limit-per-hour=500

# LLM Fallback and Error Handling
prediction.llm.fallback-to-mock=true
prediction.llm.fail-fast=false

# LLM Cost Management
prediction.llm.max-cost-per-request=0.10
prediction.llm.daily-cost-limit=10.00

# LLM Caching
prediction.llm.enable-caching=true
prediction.llm.cache-ttl-hours=24

# LLM Batch Processing
prediction.llm.batch-mode=true
prediction.llm.max-batch-size=5
prediction.llm.batch-timeout-seconds=60
prediction.llm.async-timeout-seconds=120

# LLM Monitoring and Logging
prediction.llm.enable-metrics=true
prediction.llm.log-requests=true
prediction.llm.log-responses=false

# Gemini Native Integration Configuration
# Enable/disable the native Gemini integration (replaces LangChain4J-based integration)
prediction.gemini-native.enabled=true
# Google GenAI API key - set via environment variable GEMINI_API_KEY
prediction.gemini-native.api-key=${GEMINI_API_KEY:}
# Gemini model to use for predictions (gemini-1.5-flash, gemini-1.5-pro, etc.)
prediction.gemini-native.model=gemini-2.5-flash
# Maximum number of articles to process in a single batch (1-100)
prediction.gemini-native.max-batch-size=20
# Interval in seconds for polling batch job status (5-300)
prediction.gemini-native.polling-interval-seconds=10
# Maximum number of retry attempts for failed requests (0-10)
prediction.gemini-native.max-retries=3
# Timeout in minutes for batch job completion (5-120)
prediction.gemini-native.timeout-minutes=30

# Batch Processing Configuration
# Enable/disable batch processing capabilities
# Temporarily disabled due to quota limits on batch API
# The Google GenAI Batch API may have different quota limits or may not be available
# for all API key tiers. When disabled, the system will use individual API calls
# which should work with standard quota limits.
# To re-enable: set prediction.gemini-native.batch.enabled=true
prediction.gemini-native.batch.enabled=false
# Maximum number of concurrent batch jobs (1-10)
prediction.gemini-native.batch.max-concurrent-jobs=3
# Initial retry delay in seconds for failed batch operations (1-60)
prediction.gemini-native.batch.retry-delay-seconds=5
# Maximum retry delay in seconds with exponential backoff (60-600)
prediction.gemini-native.batch.max-retry-delay-seconds=300

# Monitoring Configuration
# Enable/disable monitoring and health checks
prediction.gemini-native.monitoring.enabled=true
# Enable/disable metrics collection
prediction.gemini-native.monitoring.metrics-enabled=true
# Enable/disable health check endpoints
prediction.gemini-native.monitoring.health-check-enabled=true

# Analysis job configuration
prediction.analysis.max-concurrent-jobs=3
prediction.analysis.job-timeout-minutes=30

# Cleanup Configuration
analysis.cleanup.enabled=true
analysis.cleanup.retention-days=30
analysis.cleanup.batch-size=100

# WebSocket configuration
quarkus.websockets.max-frame-size=1048576

# Logging configuration
quarkus.log.level=INFO
quarkus.log.category."ai.falsify.prediction".level=DEBUG
quarkus.log.category."ai.falsify.crawlers.common".level=DEBUG
quarkus.log.category."dev.langchain4j".level=DEBUG
quarkus.log.category."root".level=INFO

# Console logging format for better debugging
quarkus.log.console.format=%d{HH:mm:ss} %-5p [%c{2.}] (%t) %s%e%n
quarkus.log.console.level=DEBUG

# Test configuration (enable dev services for tests)
%test.quarkus.datasource.devservices.enabled=true
%test.quarkus.datasource.db-kind=h2
%test.quarkus.datasource.jdbc.url=jdbc:h2:mem:testdb;DB_CLOSE_DELAY=-1
%test.quarkus.hibernate-orm.enabled=true
%test.quarkus.hibernate-orm.database.generation=drop-and-create
%test.quarkus.redis.devservices.enabled=true
%test.crawler.common.redis.enable-redis=true

# Integration test configuration (enable services for IT tests)
%integration-test.quarkus.datasource.devservices.enabled=true
%integration-test.quarkus.datasource.db-kind=h2
%integration-test.quarkus.datasource.jdbc.url=jdbc:h2:mem:testdb;DB_CLOSE_DELAY=-1
%integration-test.quarkus.hibernate-orm.enabled=true
%integration-test.quarkus.hibernate-orm.database.generation=drop-and-create
%integration-test.quarkus.redis.devservices.enabled=true
%integration-test.crawler.common.redis.enable-redis=true